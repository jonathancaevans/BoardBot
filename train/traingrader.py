# -*- coding: utf-8 -*-
"""scrrouteclass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l_-QI1OBYBLnWSEhqacVUnFxjdTK8Tg5
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model

import pandas as pd
import numpy as np

from IPython.display import Image

from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt

print(tf.__version__)

import numpy as np

loadedArr = np.loadtxt("/content/drive/MyDrive/data/gcr/climbs.csv", delimiter=",")
images = loadedArr.reshape((-1,36,18,4))

labels = np.loadtxt("/content/drive/MyDrive/data/gcr/grades.csv", delimiter=",")

angles = np.loadtxt("/content/drive/MyDrive/data/gcr/angles.csv", delimiter=",")

print(images.shape, labels.shape, angles.shape)

labels = labels/23

def buildNetwork():
  visible1 = layers.Input(shape=(36, 18, 4))

  conv1 = layers.Conv2D(8, (5, 5), strides=(1, 1), padding='same')(visible1)
  leaky1 = layers.LeakyReLU(.2)(conv1)
  dropout1 = layers.Dropout(0.15)(leaky1)

  conv2 = layers.Conv2D(16, (5, 5), strides=(3, 3), padding='same')(dropout1)
  leaky2 = layers.LeakyReLU(.2)(conv2)
  dropout2 = layers.Dropout(0.15)(leaky2)

  conv3 = layers.Conv2D(32, (5, 5), strides=(1, 1), padding='same')(dropout2)
  leaky3 = layers.LeakyReLU(.2)(conv3)
  dropout3 = layers.Dropout(0.15)(leaky3)

  conv4 = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(dropout3)
  leaky4 = layers.LeakyReLU(.2)(conv4)
  dropout4 = layers.Dropout(0.15)(leaky4)

  conv5 = layers.Conv2D(128, (5, 5), strides=(1, 1), padding='same')(dropout4)
  leaky5 = layers.LeakyReLU(.2)(conv5)
  dropout5 = layers.Dropout(0.15)(leaky5)

  conv6 = layers.Conv2D(128, (5, 5), strides=(1, 1), padding='same')(dropout5)
  leaky6 = layers.LeakyReLU(.2)(conv6)
  dropout6 = layers.Dropout(0.15)(leaky6)

  flat1 = layers.Flatten()(dropout6)

  visible2 = layers.Input(shape=(1))

  concatenate = layers.Concatenate()([flat1, visible2])

  dense = layers.Dense(64,  activation="relu")(concatenate)

  out = layers.Dense(1,  activation="sigmoid")(dense)

  model = Model(inputs=[visible1,visible2], outputs=out)

  return model

network = buildNetwork()

plot_model(network)

network.build(input_shape=[36,18,4])
network.summary()

network.compile(optimizer=tf.optimizers.Adam(learning_rate=.0001), loss='mean_absolute_error')

history = network.fit(x = [images, angles], y = labels, epochs=35, batch_size=16)

def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  #plt.plot(history.history['val_loss'], label='val_loss')
  #plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MAE]')
  plt.legend()
  plt.grid(True)

plot_loss(history)

#.00001 .0001 more ideal test inbetween .1 to .12 at 20 BASELINE: .1010
#Changing to nonrounded dataset .12 to .1033 Change back to .0001 lr and get baseline
#Change the epochs to 16? Speed at 5ms a step with no multiprocesssing
#Keep .0001 lr, up epochs to 25
#Looks good mess with batch size down to 8 fail validation unstable need to add angle label
#half dropout from .3 .15 .0987
#half droupout again .15 -> .075 extend to 35 0.0995 fail
#0.0992
#Backdown to 18
#Addition of angle brings us down to .0816
#Bring up epochs to 40 back down to 35 .0816
#trying leakyrelu .2 Seems to like this better we'll stay with this for now.
#Trying MSE, No luck still bumpy
#Going to try nixing an exit CONV Layer see where we get Ehh .0829 see some step off in curve but not
#significant might as well move to working with keras tuner will try switching dilations sizes see where it goes
#Close mostly the same .0819 changing back
#Ok I was wrong left out last conv block reswitching testing for diff .0831
#Still not ideal going to change it back hpo doesnt work changing kernal sizes from 5x5 to 6x6 0.0837
#No work down to 4x4? 8-9 ms might make us faster, not faster we'll see on performance 0.0821 not anything
#switch back up last layer to 256 0.0827 no diff changing back
#Accidentally set the output to relu and the dnese to sigmoid switched testing: .0779
#injecting metadata into the training earlier in the process is a fail like .9, Mabye not enough metadata
#Lets try upping to 128 on the output dense layer: 0.0806 no sig diff move back to 64 retest for assurance

!pip install tensorflowjs

network.save("grader", overwrite = True, include_optimizer = False, save_format = "h5")
!tensorflowjs_converter --input_format keras ./grader ./graderjs